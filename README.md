# Tabular-Playground-May-Kaggle-Competition
Kaggle Competition in which 50 features were used to try and predict the probability of a target variable being in each of the four possible classes and the accuracy of my predictions were judged using the log loss method. I tried a variety of different models starting with neural networks, but after trying both a regular neural network model and a convolutional neural network model my best log loss score was in the 1.10 - 1.09 range. Next I tried some boosting models. XGBoost immediatly showed better results than the neural network models as it consistently produced a log loss in the 1.09 - 1.088 range. The best model I found to be was the CatBoost model which ended up producing my best log loss score of 1.082. I attempted to use principal component analysis to try and reduce the dimension of the model, however all of my PCA models produced lower scores than when I simply left all the features in the model. I also tried using a stacking classifier, but the results were not better than simply using the CatBoost model.
